\documentclass[22pt]{beamer}
\usepackage{textcomp}
\usepackage[orientation=portrait, size=custom, width=91.44, height=91.44,scale=1.2]{beamerposter} % 36in*2.5 = 90cm
\usepackage[absolute,overlay]{textpos}
\usepackage{bookmark} %pdflatex says to use this to avoid errors...
\usepackage{graphicx} %for including images
\graphicspath{{figs/}} %location of images
\usepackage{wrapfig} %wrap text around the images
\usepackage{listingsutf8}    
\usepackage{amsmath}
\usepackage{gensymb}
\usepackage[export]{adjustbox}
\usepackage[skins,theorems]{tcolorbox}
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
\usepackage{array}
\usepackage{booktabs,adjustbox}
\usepackage{subcaption} 


\usetheme{Berlin}
\definecolor{MacBlue}{rgb}{0.10196,0.22353,0.53725}
\definecolor{MacMaroon} {rgb}{0.47843, 0, 0.23137}
\definecolor{MacMaroon2} {rgb}{0.47451, 0, 0}
\definecolor{MacGray}{rgb}{0.50196,0.49804,0.51765}
\definecolor{MacMaroon3}{rgb}{00.47,0.2,0.31}
\definecolor{MacGold}{rgb}{1, 0.75,0.35}
\usecolortheme[named=MacBlue]{structure}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{navigation symbols}{}

\title{Approximate Regular Expressions: A Comparison of Exact and Approximate Matching Methods}
\subtitle{}  
  \author[Gadriwala, Noshin \& Siddiqui]{Umme Salma Gadriwala, Tasnim Noshin, Rumsha Siddiqui \vspace{0.3cm} \newline \small \{gadriway, noshint, siddiqur\}@mcmaster.ca}
  \institute[McMaster University]{Department of Computing and Software, McMaster University

1280 Main St. W, Hamilton, Ontario, Canada L8S 4L8}
  \date{April 2019}

\begin{document}
\begin{frame}[fragile]

\begin{textblock}{2}(0.7,1)
\includegraphics[height=8.5cm]{englogo.png} % We can use CAS logo as well? 
\end{textblock}

\begin{textblock}{8}(4,1)
\titlepage
\end{textblock}

\begin{textblock}{2}(12.7,1)
\centering
\textbf{Tweet us} \\
your thoughts: \\
{[worth pursuing?] [achievable?]}
{\color{MacBlue} @TeamDeepCheck \#MacCSCapstone \#MacEng}
\end{textblock}

\begin{textblock}{7.25}(0.5,3.1)

\begin{block}{Motivation}

\end{block}

\begin{block}{Problem}

\end{block}

\begin{block}{Solution}

\end{block}


\begin{block}{Background Study}

\vspace{5mm} %5mm vertical space

\textbf{Adversarial Samples:} Neural networks are vulnerable to adversarial samples. Small changes to the input vector can result in misleading activations and a final misclassification. This makes it difficult to apply neural networks in security-critical areas. Development of neural network defenses is a growing area of research. 

\begin{figure}
\includegraphics[scale=2.6]{Panda1.PNG}
\end{figure}

\vspace{5mm} %5mm vertical space

\end{block}
\end{textblock}


\begin{textblock}{7.25}(8.25,3.1)
\begin{block}{Methodology}
In our proposed evaluation metric, we compare the accuracy of a given neural network's output of an untampered sample set to that of an adversarial sample set. For each adversarial attack, we plan to conduct the following:
\begin{enumerate}
\item Determine the accuracy of a given neural network on an untampered test set by identifying the percentage of outputs that are true positive (TP), false positive (FP), true negative (TN) and false negative (FN).
\item Run an adversarial attack on the untampered test set to generate an adversarial sample set. Determine the accuracy of the neural net when running the adversarial sample set by identifying the percentage of outputs that are TP, FP, TN and FN.
\item Report the difference in accuracy percentages between the adversarial and untampered test sets in a confusion matrix.
\item Run a defense on the neural network. Determine the accuracy percentages of the defended neural net when inputting the adversarial sample set. 
\item Report the difference in accuracy percentages between the defended and non-defended neural network when running the adversarial samples. 
\end{enumerate}

\begin{figure}
\includegraphics[scale=2]{Table_colour_longer.png}
\end{figure}

The reported percentages in each quadrant of the matrix can be used by deep learning developers to analyze potential changes in accuracy, sensitivity, specificity and precision. 
\end{block}


\begin{block}{Conclusions \& Future Work}
The proposed metric allows us to evaluate the effects of various adversarial attacks and potential neural network robustness using known defences. In the next phase of our work, we will implement this metric to explore the vulnerabilities of neural networks using Keras and Tensorflow. We further hope to conduct a survey of deep learning researchers' perspectives on neural net verification to explore the sociological views of using deep learning models in security critical systems.
\end{block}


\begin{block}{Acknowledgements}
We thank Dr. R. Samavi and Yifan Ou for the opportunity and support in exploring security in deep learning machine models, and Dr. C. Anand for his continued support and guidance in our learning and endeavours.
\end{block}

%--------------------------------------------
%REFERENCES
\begin{block}{References}
\setbeamertemplate{bibliography item}{\insertbiblabel}
\bibliographystyle{ieeetr}
{\scriptsize
\bibliography{bib}}
\end{block}

\begin{comment}
\begin{block}{References}
\setbeamertemplate{bibliography item}{\insertbiblabel}
\bibliographystyle{ieeetr}
{\scriptsize
\bibliography{bib}}
\end{block}
\vspace{-1.8mm}
\end{comment}
\end{textblock}

%
%\begin{textblock}{7.5}(0.5,14.6)
%\centering
%\textbf{Tweet us} \\
%your thoughts on DeepCheck and where you think the future of neural networks is headed \\
%{\color{MacBlue} @TeamDeepCheck \#MacCSCapstone \#MacEng}
%
%\end{textblock}

\end{frame}
\end{document}
